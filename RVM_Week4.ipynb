{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "#@title Step -1: Mount drive\n",
        "#@markdown Run this cell. If prompted, press \"Connect to Google Drive\" and select your Google account.\n",
        "#@markdown Then, under the folder icon üìÅ on the left panel, you should see the folder **drive** appear.\n",
        "from google.colab import drive\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import os, sys\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "try:\n",
        "  drive.mount('/content/drive', force_remount=False)\n",
        "  # os.chdir('/content/drive/My Drive/DLE-Feb23/Projects')\n",
        "  sys.path.append('/content/drive/My Drive/DLE-Feb23/Projects')\n",
        "  os.chdir('/content/drive/MyDrive/Colab Notebooks/')\n",
        "  display(\"‚≠ê Mounted successfully!\")\n",
        "except:\n",
        "  display(HTML('<span style=\"color:red\">An error occurred. Try again!</span>'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "j28iNPic6CSz",
        "outputId": "91e6e064-719b-47b9-e783-2fc93a27ed92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'‚≠ê Mounted successfully!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 0: Import Packages\n",
        "%%capture\n",
        "!pip install openai\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install transformers\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import openai\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from dle_utils.dle_utils import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import transformers\n",
        "import ast\n",
        "import pprint"
      ],
      "metadata": {
        "id": "nydNA-9-ISLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Accessing Open AI's API\n",
        "\n",
        "Before starting this assignment, you'll need to do the following steps:\n",
        "\n",
        "1. [Create an account on OpenAI](https://auth0.openai.com/u/signup/identifier?state=hKFo2SBzQm1sd2pOTUJ2SG9sSHBBdUU1bGNpdllveFhrWW8wc6Fur3VuaXZlcnNhbC1sb2dpbqN0aWTZIHctYmtFZ3h0ck9saDNVWFhUTnRmckM5azd0RjUtMjMwo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) (if you have not done so already).\n",
        "\n",
        "2. In the upper right hand corner, click on your profile icon and select  **View API keys**. On the API keys page, copy your API key into the cell below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ctYY7b1TqauMobC6J-Vm_VGBF9nv_4XI\" width=800/>\n",
        "\n",
        "You will only see this key once, so make sure to copy it in a safe place!\n",
        "\n",
        "3. On the account dropdown, go to **Manage account** to make sure you have access to credits in your account. If you started a free trial, you should have $5.00. If you're like me and your credits have expired/run out, you may want to create a new account! (Otherwise, ChatGPT API access is quite cheap, so you may want to consider adding payment information)\n",
        "\n",
        "**Remember to remove your API key from the homework before submitting it!**\n"
      ],
      "metadata": {
        "id": "jHYQFbSP7vJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put down your API key here\n",
        "\n",
        "openai.api_key = ..."
      ],
      "metadata": {
        "id": "I1WXEXEqgZkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep learning APIs\n",
        "\n",
        "In a not so distance past, loading and using deep learning models was a laborious and difficult task, involving lots of helper functions and niche knowledge about how models worked.\n",
        "\n",
        "Now, thanks to the democratization of AI, the most powerful models are available through APIs. In this assignment, we'll give a quick tutorial on the most recent and exciting API: Open AI's ChatGPT API. Then, we'll go through a real-world example on how they might be used to help you in your own work.\n",
        "\n",
        "### Using OpenAI's API\n",
        "\n",
        "OpenAI's API is really accessible via Python. There are two basic modes in this API.\n",
        "\n",
        "1. ```openai.Embedding```\n",
        "2. ```openai.ChatCompletion```\n",
        "\n",
        "Roughly speaking, these two modes are just the two levels of abstraction. At the first level, the Embedding API gives you the most raw form of the model -- the embedding vectors of a given text. Next, the ChatCompletion API is a higher level version of the embedding API, where the model has been trained to interpret instructions. (By the way, there are other modules like [Audio](https://platform.openai.com/docs/guides/speech-to-text) which are really cool, but we wont' cover here!)"
      ],
      "metadata": {
        "id": "oQKRoRAZ9us9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "The first use case is in extracting model embeddings. This is identical to the model embeddings used in week 2's assignment. Here, we'll just do a quick demo in terms of using these embeddings for a downstream task.\n",
        "\n",
        "In the cell below, load ```local_df```. In this, you'll see 1000 genuine Google Local reviews from places in Washington, D.C. Each review is associated with a rating from 1-5."
      ],
      "metadata": {
        "id": "np1oTA_FPUKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_df = pd.read_csv('../DLE-Feb23/Projects/dle_utils/data/google_local_data.csv', index_col=0)\n",
        "local_df.head()"
      ],
      "metadata": {
        "id": "W3j_d6XRLCOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say that Google users have a tendency to forget to put a 1-5 star rating next to their review. Can we use the embedding API to predict which rating should be associated with each review?\n",
        "\n",
        "### Extracting embeddings\n",
        "\n",
        "Use ```openai.Embedding.create()``` to extract embeddings for each review in ```local_df```.  ```openai.Embedding.create()``` takes in two parameters: ```model``` and ```input```. (You can read embeddings [here](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)).\n",
        "\n",
        "For ```model```, use ```\"text-embedding-ada-002\"```, which is the best, fastest, and cheapest ($0.0004 / 1k tokens).\n",
        "\n",
        "For ```input```, you just need to enter the text you want to embed as a string.\n",
        "\n",
        "In the cell below, take the first review from ```local_df``` and extract its embedding.\n",
        "\n",
        "(Hint: For a Pandas dataframe, you can access the text from a specific row with ```local_df.iloc[i]['text']```, where ```i``` is the index of the row."
      ],
      "metadata": {
        "id": "B_Sdy9Dsctdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate response with the embedding of the first row\n",
        "sample_review = ...\n",
        "\n",
        "# Extract embedding\n",
        "response = ...\n",
        "\n",
        "print(response.keys())"
      ],
      "metadata": {
        "id": "NI0spMxFgdIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('4.1.1', response)"
      ],
      "metadata": {
        "id": "RWqmKAPTeCMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output, ```response```, is accessible as a dictionary.\n",
        "\n",
        "Find where the embeddings are stored in this dictionary. (Hint: You can get the keys to a dictionary with ```.keys()```). The embeddings will be a list of lists, which we want to convert to a numpy array."
      ],
      "metadata": {
        "id": "LX7QLySdeBta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the embeddings from the response and convert to a numpy array\n",
        "\n",
        "emb_arr = ..."
      ],
      "metadata": {
        "id": "iAr1X_gcgfYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('4.1.2', emb_arr)"
      ],
      "metadata": {
        "id": "Cqvfz503hHF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now that we can extract the embedding for any piece of text, we want to get the embeddings for a subset of ```local_df```.\n",
        "\n",
        "You'll notice the shape of ```emb_arr``` is ```(1536,)```, which is the number of dimensions each embedding is.\n",
        "\n",
        "In the cell below, populate the list ```sample_emb``` with the embeddings for the first **20** reviews. Then, convert ```sample_emb``` into a numpy array. The shape of ```sample_emb``` should be ```(20, 1536)```.\n",
        "\n",
        "This should not take more than a couple of minutes.\n",
        "\n",
        "(You may get a notice like ```RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists.``` If this happens, you may need to wait a few more minutes. It may also help to use a [sleep](https://www.programiz.com/python-programming/time/sleep) timer to keep under the rate limit.)"
      ],
      "metadata": {
        "id": "-J4bIN9uiAuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings for 20 reviews in local_df\n",
        "# Populate sample_emb with a numpy array of embeddings\n",
        "\n",
        "def get_embedding(input_string):\n",
        "  ...\n",
        "\n",
        "sample_emb = ..."
      ],
      "metadata": {
        "id": "0KZMlqRDggzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('4.1.3', sample_emb)"
      ],
      "metadata": {
        "id": "QqpiXK10mq2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Next, we want to gather emebeddings for all 1000 reviews. If you have an OpenAI API account that is older than 48 hours old and you have your credit card attached, you can query 1000 reviews easily. However, newer accounts are rate limited. If you find yourself rate-limited, just skip past this next part and we'll provide you the embeddings for the full 1000 as a ```.npy``` file.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1exdv5DIvWvw9-0O8xYazttEyiuFcdzDg\" width=250/>."
      ],
      "metadata": {
        "id": "Tu6f9ngWEM0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings for all reviews in local_df\n",
        "# Populate local_emb with a numpy array of embeddings\n",
        "# Do this part if you aren't rate-limited!\n",
        "\n",
        "def get_embedding(input_string):\n",
        "  ...\n",
        "\n",
        "local_emb = ..."
      ],
      "metadata": {
        "id": "ReZCdOUuxM3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are rate-limited, just run this!\n",
        "\n",
        "local_emb = np.load('../DLE-Feb23/Projects/dle_utils/data/local_emb.npy')"
      ],
      "metadata": {
        "id": "Wt89Ot4WpTSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a dataset of 1000 embeddings, let's see if we can train a machine learning model to predict the ratings. We can start with doing a binary prediction of whether the review was a low rating (1, 2, or 3) or a high rating (4 or 5).\n",
        "\n",
        "The cell below has been given to you to train a basic model on this binary task."
      ],
      "metadata": {
        "id": "dRmvkYihlobF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run this!\n",
        "y = (local_df['rating'] > 3).astype(int).to_list()\n",
        "X = local_emb\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42).fit(X_train, y_train)\n",
        "\n",
        "print(f'Model AUC: {roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])}')"
      ],
      "metadata": {
        "id": "nl0SkjlvOWx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad, right? An AUC of 0.95 is pretty impressive.\n",
        "\n",
        "Even though OpenAI's API is pretty cheap, using it across a lot of queries could still cost us some serious dough. Fortunately, free versions of large language models exist. Even better news is that they are incredibly easy to use. Run the line below to load up one of these models."
      ],
      "metadata": {
        "id": "Fkucr_NTOjhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipeline = pipeline('feature-extraction', model='xlnet-base-cased')"
      ],
      "metadata": {
        "id": "LfN1ZAZIOpcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function ```pipeline``` takes in a string and returns the embedding. However, it will give one embedding for each token, so you'll have to take the average over all tokens like you did in the week 2 assignment.\n",
        "\n",
        "One advantage of using Huggingface is that it's totally free -- no rate limits :)\n",
        "\n",
        "In the cell below, extract all the embeddings using ```pipeline``` and store the embeddings as a numpy array in ```local_emb_hf``` (hf stands for huggingface). After doing so, run the machine learning model and see how well these embeddings perform on this task. (It might take a couple minutes!)\n"
      ],
      "metadata": {
        "id": "F0gYfPo9ryiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings for all reviews in local_df\n",
        "# Populate local_emb_hf with a numpy array of embeddings\n",
        "\n",
        "local_emb_hf = ..."
      ],
      "metadata": {
        "id": "dkF8gnAegli7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('4.1.4', local_emb_hf)"
      ],
      "metadata": {
        "id": "xmXbOoSTkr6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = (local_df['rating'] > 3).astype(int).to_list()\n",
        "X = local_emb_hf\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42).fit(X_train, y_train)\n",
        "\n",
        "print(f'Model AUC: {roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])}')"
      ],
      "metadata": {
        "id": "SOEOpfo0pDPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did this right, you should be seeing an AUC of around 0.89. It's not bad, but certaintly not as good as 0.95. As you can see, performance comes with a price.\n",
        "\n",
        "If you'd like, you can try a couple other models to see how well they do. [List of huggingface models](https://huggingface.co/models).\n",
        "\n",
        "**Challenge question: Can you find a huggingface model that performs at least 0.90 AUC?**\n",
        "\n",
        "At the end of the day, whether you use OpenAI's API or a free model from huggingface, the choice just depends on what task you need them for.\n",
        "\n",
        "___\n",
        "\n",
        "But what about tasks which we don't have a label for? Suppose we wanted to get the predicted reviews, but we don't have training data? A few years ago, we would have been stuck trying to gather more data. However, now with OpenAI's chat completion API, we may be able to do more than we think.\n",
        "\n",
        "## Chat Completion\n",
        "\n",
        "Chat completion provides a whole new dimension of functionality on large language models. Whereas embeddings are hard to interpret machine language, the chat completion API allows us to give the model instructions on how to interpret the prompt. To illustrate this, we will be working with real reviews of restaurants from Google. Run the cell below to read in the data."
      ],
      "metadata": {
        "id": "VgAJPe3VsJHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df = pd.read_csv('../DLE-Feb23/Projects/dle_utils/data/review_df_cols.csv', index_col=[0])\n",
        "review_df.head()"
      ],
      "metadata": {
        "id": "_IEMpzaugpdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice three columns: ```business_id, review_text, rating```. The business is a unique id associated with each restaurant. The rating is a score from 1-5, and the review_text is, well, the review text.\n",
        "\n",
        "Rather than try to predict ratings, we're going to try something a little more complicated. Imagine you're working for a restaurant recommendation company, and this user feedback comes in. Your manager wants you to solve this problem and you're tasked with finding the answer using deep learning:\n",
        "\n",
        "**I love using your app to find restaurants, but each time I get to a restaurant, I don't know what to order. I know that other users write reviews about specific menu items, but it's a time-consuming process to go through each review to find them. I wish there was a quick summary of the reviews for each menu item!**\n",
        "\n",
        "Let's break down this problem into a couple steps:\n",
        "\n",
        "1. We want to collect all the reviews for a specific restaurant. For example, we can look at the restaurant with ```business_id=605618c0d335d0abfb415a0f```. Run the cell below to get a sample of the reviews:"
      ],
      "metadata": {
        "id": "G7JTD2BQtzOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run this!\n",
        "reviews = review_df[review_df['business_id']=='605618c0d335d0abfb415a0f']['review_text'].to_list()\n",
        "print('\\n'.join(reviews))"
      ],
      "metadata": {
        "id": "XAnEWzWdEXTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. For each review, we want to ask ChatGPT to extract the reviews for specific menu items. There are a few formats we can get this in, but to make it clear, we can have ChatGPT return it as a json/dictionary.\n",
        "\n",
        "The idea is to get a lookup table of items, plus their reviews. For example,\n",
        "\n",
        "```{'chow mein': 'Most users found it delicious, with some thinking it is too spicy', \\\n",
        "    'orange chicken': 'This is a crowd favorite, and has been described as \"tangy\" and \"zesty\"'}```\n",
        "\n",
        "Let's start with one of the reviews:"
      ],
      "metadata": {
        "id": "9Q_AYrmUwlB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run this!\n",
        "sample_review = reviews[8]\n",
        "print(sample_review)"
      ],
      "metadata": {
        "id": "rfjfvedipg71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to get ChatGPT to convert this review to a dictionary:\n",
        "\n",
        "To query OpenAI's ChatGPT API, use ```openai.ChatCompletion.create()```. You need to pass two parameters:\n",
        "\n",
        "1. ```model```, which should be 'gpt-3.5-turbo'\n",
        "2. ```messages```, which should be a list of dictionaries. Each dictionary should contain two entries: ```'role'``` and ```'content'```.\n",
        "\n",
        "```'role'``` can either be ```'system'```, ```'user'```, or ```'assistant'```.\n",
        "\n",
        "The role ```'system'``` is meant to be a persistent instruction for the prompt. You can write something like ```'You are an restaurant review writer whose job it is to help people find good menu items. You will return responses is json dictionary format only'```.\n",
        "\n",
        "The role ```'user'``` is the prompt given. For example, the prompt can be: ```'The following text is a list of restaurant reviews. For each unique menu item mentioned, give a summary of the user reviews for that menu item. Return the output in the form of a json dictionary, where the keys are unique menu items and the values are the summary.'```\n"
      ],
      "metadata": {
        "id": "TfMpPXd9yEtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these lines\n",
        "system_prompt = 'You are an restaurant review writer whose job it is to help \\\n",
        "               people find good menu items. You will return responses is json \\\n",
        "                dictionary format only'\n",
        "\n",
        "# Feel free to modify this!\n",
        "prompt = f'The following text is a list of restaurant reviews. \\\n",
        "        For each unique menu item mentioned, give a summary of \\\n",
        "        the user reviews for that menu item. \\\n",
        "        Return the output in the form of a json dictionary, \\\n",
        "        where the keys are unique menu items \\\n",
        "        and the values are the summary. : \"{sample_review}\"'\n"
      ],
      "metadata": {
        "id": "wh2jiiGa0qKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```.create``` function expects the parameter ```messages``` to be a list, with each element as a dictionary which specifies who (```role```) and what (```content```) is said.\n",
        "\n",
        "So for example, if we wanted to ask ChatGPT what the first US state is, we could pass in the following into ```messages```:\n",
        "\n",
        "```messages=[{'role': 'system', 'content': 'You are a helpful tour guide.'}, {'role': 'user', 'content': 'What is the first US state?'}]```\n",
        "\n",
        "In the cell below, pass the example about US states into ```openai.ChatCompletion.create``` and print out what the output is in ```response```:"
      ],
      "metadata": {
        "id": "TiMZMeJg3pJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ...\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "l7T7QbLXgw54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the response itself is a dictionary. Here are a couple key things to point out about this response:\n",
        "\n",
        "1. The actual response is found in \"content\" under \"choices\".\n",
        "2. The \"usage\" dictionary may be helpful for understanding how many credits you have used per query (ie. for logging purposes). But don't worry about that for this project.\n",
        "\n",
        "With this information, we are ready to get ChatGPT to convert a restaurant review into a menu item-specific look-up table.\n",
        "\n",
        "In the cell below, query the API given the variables ```prompt``` and ```system_prompt``` defined for above restaurant reviews:"
      ],
      "metadata": {
        "id": "Vy3aOSCF7Jkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ..."
      ],
      "metadata": {
        "id": "NLU_sDQzgzWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, extract just the content. You will notice it is a string with ```\\n``` and ```\\```. You can turn this into a Python object using ```ast.literal_eval```. This turns a string of a dictionary into an actual dictionary.\n",
        "\n",
        "In the cell below, extract the API's response content and convert it into a Python dictionary using ```ast.literal_eval```. (You may need to navigate the ```response``` dictionary a bit to find what you want)"
      ],
      "metadata": {
        "id": "vf62KnIP-Rdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_as_dict = ...\n",
        "\n",
        "print(f' Original review: {sample_review}')\n",
        "pprint.PrettyPrinter().pprint(response_as_dict)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "VhxMZSsDg1c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('4.1.5', response_as_dict)"
      ],
      "metadata": {
        "id": "WA575Fgx_Apv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does this summary make sense?\n",
        "\n",
        "Next, let's try doing this for multiple reviews.\n",
        "\n",
        "In the cell below, extend the prompt to include all the reviews in the list ```reviews```. You have the choice of the following:\n",
        "\n",
        "1. Combine all the reviews into one giant string and combine that with your original prompt\n",
        "\n",
        "2. Run the prompt once for each review, and then merge the dictionaries into one\n",
        "\n",
        "3. Do the reviews in chunks (a mix between one and two).\n",
        "\n",
        "A few hints: (1) If you are running into token limits from OpenAI, you will need to find a way to divide your prompt into smaller bits and combinin them after. (2) Always be as explicit with the API as possible! (Tell it what you are putting in as input, and what you expect to receive as output).\n",
        "\n"
      ],
      "metadata": {
        "id": "gsUKuqd5-5xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these lines\n",
        "system_prompt = ...\n",
        "\n",
        "prompt = ..."
      ],
      "metadata": {
        "id": "4mb92Yc9oTfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response\n",
        "response = ...\n",
        "# Store as a dictionary\n",
        "response_as_dict = ...\n",
        "\n",
        "print(f' Original review: {\"|\".join(reviews)}')\n",
        "pprint.PrettyPrinter().pprint(response_as_dict)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "iueqAToxg6Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should have a much longer dictionary, with many more menu items now!\n",
        "\n",
        "You may have noticed if you called the API multiple times that each call yielded a different result. This is due to the inherent randomness in the model's predictions.\n",
        "\n",
        "Let's investigate the sources of randomness a little bit here.\n",
        "\n",
        "In the cell below, generate a second summary of the reviews and store it in the variable ```response_as_dict2```.\n",
        "\n",
        "This time, add in as a parameter to ```openai.ChatCompletion.create``` the variable ```temperature```. In general, ```temperature``` should be between 0 and 2, with 0 being the least amount of randomness and 2 being the greatest amount of randomness. Choose a temperature of 1.1 and see what the output looks like. (Hint: If the output cannot be interpreted as a dictionary, try running it again, or change the prompt!)\n",
        "\n"
      ],
      "metadata": {
        "id": "LqTdIvWGH7Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response again but with temperature as 1.1\n",
        "response = ...\n",
        "\n",
        "# Store as a dictionary\n",
        "response_as_dict2 = ..."
      ],
      "metadata": {
        "id": "5zQMtpFwg-Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice about the output? While each run is different, you may see the following: (1) different menu items, (2) different styles of description for each menu item, or (3) a different format alltogether.\n",
        "\n",
        "In practice, it is good to get a sample of the types of outputs being produced. Another parameter to use is ```n```, which just controls the number of samples you want ChatGPT to produce.\n",
        "\n",
        "In the cell below, rather than changing temperature, just set ```n=2```."
      ],
      "metadata": {
        "id": "-Zw0DJBReZj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response again but with n=2\n",
        "response = ...\n",
        "\n",
        "# Store as a dictionary\n",
        "response_run1 = ...\n",
        "response_run2 = ..."
      ],
      "metadata": {
        "id": "bJ5dWiBIhCyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at a specific menu item between the first and the second call. What kinds of differences do you notice? Are the menu items picked up by the API the same?\n",
        "\n",
        "Feel free to post your findings on #deep-learning-questions-and-debugging to see if others have gotten similar differences!\n",
        "\n",
        "This next part is pretty open-ended, so feel free to take some creative liberties. We want to use ChatGPT to filter its own outputs. What kind of prompt would you use to combine the two generated responses into a better, third output?"
      ],
      "metadata": {
        "id": "fvxacJFkasbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combine_system = ...\n",
        "combine_prompt = ...\n",
        "\n",
        "response = ...\n",
        "combined_response = ..."
      ],
      "metadata": {
        "id": "upiZW185fefi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, the strategy of using the large language model to refine itself is used quite often! While there are many possible ways to get the prompts to do what you want, here are a few guidelines provided by OpenAI: [cookbook](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)\n",
        "\n"
      ],
      "metadata": {
        "id": "MeI6sq2Hcy88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open-Ended Exploration (Optional)\n",
        "\n",
        "We hope this tutorial was helpful in understanding the ins and outs of OpenAI's ChatGPT API.\n",
        "\n",
        "There are definitely many more things we can do with this dataset, but we would love to see what you can think of.\n",
        "\n",
        "In the remaining part of this assignment, feel free to come up with a task and post your findings on Slack under #deep-learning-questions-and-debugging.\n",
        "\n",
        "Here are a few ideas:\n",
        "\n",
        "1. Look for menu items across restaurants and find the best restaurant for a specific menu item.\n",
        "\n",
        "2. Compare and contrast two businesses given their reviews.\n",
        "\n",
        "3. Find the most controversial dish!"
      ],
      "metadata": {
        "id": "Ocno9Er96cHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Step\n",
        "\n",
        "Before submitting this assignment, remember to remove your OpenAI API key from the beginning of this assignment üòÄ."
      ],
      "metadata": {
        "id": "6mQ5KegT32rn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5ybsf8M4BQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}